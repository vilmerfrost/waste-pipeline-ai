"use server";

import { createServiceRoleClient } from "../lib/supabase";
import { revalidatePath } from "next/cache";
import Anthropic from "@anthropic-ai/sdk";
import { WasteRecordSchema } from "@/lib/schemas";
import * as XLSX from "xlsx";
import { extractAdaptive } from "@/lib/adaptive-extraction"; 

const STORAGE_BUCKET = "raw_documents";

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// --- 1. UPPDATERADE S√ñKORD (Inkluderar SYSAV-termer) ---
const ALIASES = {
  weight: /vikt|m√§ngd|kvantitet|antal|netto|amount|weight/i, // "M√§ngd" f√•ngar Sysav
  cost: /belopp|pris|cost|summa|totalt|sek|kr|√†-pris/i,      // "Belopp" f√•ngar Sysav
  co2: /co2|klimat|utsl√§pp|besparing|emission/i,
  hazardous: /farligt|fa\b|asbest|lys|batteri|elavfall|elektronik/i,
  material: /material|fraktion|ben√§mning|artikel|avfallsslag/i, // "Artikelben√§mning"
  
  // NYTT: ADRESS-S√ñKORD
  address: /arbetsplats|h√§mtst√§lle|ursprung|littera|projekt|adress/i,
  receiver: /mottagare|anl√§ggning|destination/i
};

// --- 2. HJ√ÑLPFUNKTION: PARSA SVENSKA TAL ---
// Hanterar "220,00", "1 000,50" och "220.00"
function parseSwedishNumber(val: any): number {
  if (!val) return 0;
  // G√∂r om till str√§ng, ta bort mellanslag (tusenavgr√§nsare)
  let str = String(val).trim().replace(/\s/g, "");
  // Byt komma mot punkt f√∂r att g√∂ra det till "datorspr√•k"
  str = str.replace(",", ".");
  const num = parseFloat(str);
  return isNaN(num) ? 0 : num;
}

// --- 3. DEN NYA SMARTA R√ÑKNE-SNURRAN ---
function calculateBigDataTotals(sheet: XLSX.WorkSheet) {
  const json = XLSX.utils.sheet_to_json(sheet, { header: 1 }) as any[][];
  
  if (json.length < 2) return { weight: 0, cost: 0, co2: 0, hazardousCount: 0 };

  // STEG 1: HITTA RUBRIKRADEN (Den smarta delen üß†)
  // Vi scannar de f√∂rsta 30 raderna. Den rad som har flest tr√§ffar p√• v√•ra alias vinner.
  let headerRowIndex = 0;
  let maxMatches = 0;

  for (let i = 0; i < Math.min(json.length, 30); i++) {
    const rowStr = json[i].map(c => String(c).toLowerCase()).join(" ");
    let matches = 0;
    if (ALIASES.weight.test(rowStr)) matches++;
    if (ALIASES.cost.test(rowStr)) matches++;
    if (ALIASES.material.test(rowStr)) matches++;
    
    // Om vi hittar en rad med b√•de "M√§ngd" och "Artikel", d√• √§r det nog bingolotto!
    if (matches > maxMatches) {
      maxMatches = matches;
      headerRowIndex = i;
    }
  }

  console.log(`üìä Hittade tabellrubriker p√• rad: ${headerRowIndex + 1}`);

  // H√§mta rubrikerna fr√•n den vinnande raden
  const headers = json[headerRowIndex].map(h => String(h).toLowerCase());

  // STEG 2: HITTA KOLUMN-INDEX BASERAT P√Ö RUBRIKERNA
  const idx = {
    weight: headers.findIndex(h => h.match(ALIASES.weight)),
    cost: headers.findIndex(h => h.match(ALIASES.cost)), // Ofta "Belopp"
    co2: headers.findIndex(h => h.match(ALIASES.co2)),
    material: headers.findIndex(h => h.match(ALIASES.material)), // Ofta "Artikelben√§mning"
    
    // NYTT: Hitta Adress-kolumner
    address: headers.findIndex(h => h.match(ALIASES.address)),
    receiver: headers.findIndex(h => h.match(ALIASES.receiver))
  };

  let totalWeight = 0;
  let totalCost = 0;
  let totalCo2 = 0;
  let hazardousCount = 0;

  // STEG 3: LOOPA DATAN (Starta p√• raden EFTER rubrikerna)
  for (let i = headerRowIndex + 1; i < json.length; i++) {
    const row = json[i];
    
    // Safety check: Om raden √§r tom eller verkar vara en summering (inneh√•ller "Summa" eller "Totalt")
    const rowStr = row.join("").toLowerCase();
    if (!rowStr || rowStr.includes("summa") || rowStr.includes("total")) continue;

    // VIKT
    if (idx.weight !== -1) {
      totalWeight += parseSwedishNumber(row[idx.weight]);
    }

    // KOSTNAD
    if (idx.cost !== -1) {
      totalCost += parseSwedishNumber(row[idx.cost]);
    }

    // CO2
    if (idx.co2 !== -1) {
      totalCo2 += parseSwedishNumber(row[idx.co2]);
    }

    // FARLIGT AVFALL
    if (idx.material !== -1) {
      const mat = String(row[idx.material] || "");
      if (mat.match(ALIASES.hazardous)) hazardousCount++;
    }
  }

  return {
    weight: Number(totalWeight.toFixed(2)),
    cost: Number(totalCost.toFixed(2)),
    co2: Number(totalCo2.toFixed(2)),
    hazardousCount
  };
}

function extractJsonFromResponse(text: string) {
  try {
    let clean = text.replace(/```json/g, "").replace(/```/g, "");
    const firstBrace = clean.indexOf("{");
    const lastBrace = clean.lastIndexOf("}");
    if (firstBrace === -1 || lastBrace === -1) throw new Error("No JSON");
    clean = clean.substring(firstBrace, lastBrace + 1);
    return JSON.parse(clean);
  } catch (e) {
    console.error("JSON Error:", text);
    // Returnera ett korrekt strukturerat fallback-objekt som matchar schemat
    return {
      material: { value: "Kunde inte tolka", confidence: 0 },
      weightKg: { value: 0, confidence: 0 },
      cost: { value: 0, confidence: 0 },
      totalCo2Saved: { value: 0, confidence: 0 },
      date: { value: new Date().toISOString().split("T")[0], confidence: 0 },
      supplier: { value: "", confidence: 0 },
      address: { value: "", confidence: 0 },
      receiver: { value: "", confidence: 0 },
      lineItems: []
    };
  }
}

// --- UPLOAD AND ENQUEUE DOCUMENT WITH BETTER ERROR HANDLING ---
export async function uploadAndEnqueueDocument(formData: FormData) {
    const supabase = createServiceRoleClient();
    const user = { id: "00000000-0000-0000-0000-000000000000" }; 
    const file = formData.get("file") as File;
    
    // Validate file presence
    if (!file) {
      throw new Error("Ingen fil hittades i uppladdningen.");
    }
    
    // Validate file size
    if (file.size === 0) {
      throw new Error("Filen √§r tom (0 bytes). Kontrollera att filen inneh√•ller data.");
    }
    
    // Validate file size limit (50 MB)
    const MAX_SIZE = 50 * 1024 * 1024;
    if (file.size > MAX_SIZE) {
      const sizeMB = (file.size / (1024 * 1024)).toFixed(1);
      throw new Error(`Filen √§r f√∂r stor (${sizeMB} MB). Max storlek √§r 50 MB.`);
    }
    
    // Validate file extension
    const fileExtension = file.name.split(".").pop()?.toLowerCase();
    const allowedExtensions = ["pdf", "xlsx", "xls"];
    if (!fileExtension || !allowedExtensions.includes(fileExtension)) {
      throw new Error(`Filtypen "${fileExtension || 'ok√§nd'}" st√∂ds inte. Endast PDF och Excel (.xlsx, .xls) √§r till√•tna.`);
    }
    
    // Generate storage path
    const storagePath = `${user.id}/${Date.now()}-${crypto.randomUUID()}.${fileExtension}`;
    
    // Upload to storage
    const { error: uploadError } = await supabase.storage
      .from(STORAGE_BUCKET)
      .upload(storagePath, file, { cacheControl: "3600", upsert: false });
    
    if (uploadError) {
      console.error("Storage upload error:", uploadError);
      if (uploadError.message?.includes("duplicate")) {
        throw new Error("En fil med samma namn finns redan. Byt namn p√• filen och f√∂rs√∂k igen.");
      }
      if (uploadError.message?.includes("size")) {
        throw new Error("Filen √§r f√∂r stor f√∂r lagring. F√∂rs√∂k med en mindre fil.");
      }
      if (uploadError.message?.includes("quota")) {
        throw new Error("Lagringsutrymmet √§r fullt. Kontakta support.");
      }
      throw new Error("Kunde inte ladda upp filen till lagring. F√∂rs√∂k igen.");
    }
    
    // Save to database
    const { data: document, error: documentError } = await supabase
      .from("documents")
      .insert({ 
        user_id: user.id, 
        filename: file.name, 
        storage_path: storagePath, 
        status: "uploaded" 
      })
      .select()
      .single();
    
    if (documentError) {
      console.error("Database insert error:", documentError);
      // Try to clean up the uploaded file
      await supabase.storage.from(STORAGE_BUCKET).remove([storagePath]).catch(() => {});
      
      if (documentError.message?.includes("duplicate")) {
        throw new Error("Ett dokument med samma namn finns redan i systemet.");
      }
      throw new Error("Kunde inte spara dokumentet i databasen. F√∂rs√∂k igen.");
    }
    
    // Process document (don't fail the upload if processing fails)
    try { 
      await processDocument(document.id); 
    } catch (error) { 
      console.error("Process Error (upload succeeded):", error); 
      // Don't throw - the file is uploaded, processing can be retried
    }
    
    revalidatePath("/");
    revalidatePath("/collecct");
    return { message: "Uppladdat!", documentId: document.id };
}


/**
 * AI-PROCESS
 */
async function processDocument(documentId: string) {
  const supabase = createServiceRoleClient();
  const { data: doc } = await supabase.from("documents").select("*").eq("id", documentId).single();
  if (!doc) throw new Error("Dokument hittades inte");

  await supabase.from("documents").update({ status: "processing" }).eq("id", documentId);

  try {
    const { data: fileData, error: downloadError } = await supabase.storage
      .from("raw_documents")
      .download(doc.storage_path);

    if (downloadError) throw new Error("Kunde inte ladda ner fil");
    const arrayBuffer = await fileData.arrayBuffer();
    
    let claudeContent = [];
    let calculatedTotals = { weight: 0, cost: 0, co2: 0, hazardousCount: 0 };
    let isBigFile = false;

    if (doc.filename.endsWith(".xlsx") || doc.filename.endsWith(".xls")) {
      // EXCEL: Use adaptive extraction for ALL rows from ALL sheets
      const workbook = XLSX.read(arrayBuffer);
      
      // ‚úÖ FIX: Process ALL sheets, not just the first one!
      console.log(`üìä Excel has ${workbook.SheetNames.length} sheet(s): ${workbook.SheetNames.join(', ')}`);
      
      let allData: any[][] = [];
      
      for (const sheetName of workbook.SheetNames) {
        const sheet = workbook.Sheets[sheetName];
        const sheetData = XLSX.utils.sheet_to_json(sheet, { header: 1, defval: "" }) as any[][];
        
        if (sheetData.length === 0) continue;
        
        console.log(`   üìÑ Sheet "${sheetName}": ${sheetData.length} rows`);
        
        if (allData.length === 0) {
          allData = sheetData;
        } else {
          // Skip header row on subsequent sheets if it looks like a header
          const firstRowLooksLikeHeader = sheetData[0]?.some((cell: any) => 
            String(cell).toLowerCase().match(/datum|material|vikt|adress|kvantitet/)
          );
          allData = [...allData, ...(firstRowLooksLikeHeader && sheetData.length > 1 ? sheetData.slice(1) : sheetData)];
        }
      }
      
      const jsonData = allData;
      console.log(`üìä Using adaptive extraction for ${jsonData.length} rows from all sheets...`);
      
      // Get settings (or use defaults)
      const { data: settingsData } = await supabase
        .from("settings")
        .select("*")
        .single();
      const settings = settingsData || {};

      // Use adaptive extraction for ALL rows
      const adaptiveResult = await extractAdaptive(
        jsonData as any[][],
        doc.filename,
        settings
      );

      // Convert to expected format
      const extractedData = {
        ...adaptiveResult,
        totalCostSEK: 0,
        documentType: "waste_report",
      };

      // Determine status based on extraction quality
      const qualityScore = (adaptiveResult._validation.completeness + (adaptiveResult.metadata?.confidence || 0) * 100) / 2;
      const status = qualityScore >= 90 ? "approved" : "needs_review";
      
      await supabase.from("documents").update({
        status,
        extracted_data: extractedData,
        updated_at: new Date().toISOString()
      }).eq("id", documentId);

      revalidatePath("/");
      return;

    } else {
      // PDF: Keep existing Claude Vision processing with logging
      const processingLog: string[] = [];
      const log = (msg: string) => {
        const ts = new Date().toISOString().split('T')[1].split('.')[0];
        processingLog.push(`[${ts}] ${msg}`);
        console.log(msg);
      };
      
      log(`${"=".repeat(60)}`);
      log(`üìÑ PDF EXTRACTION: ${doc.filename}`);
      log(`${"=".repeat(60)}`);
      
      const base64Pdf = Buffer.from(arrayBuffer).toString("base64");
      log(`‚úì PDF converted to base64 (${(arrayBuffer.byteLength / 1024).toFixed(0)} KB)`);
      
      claudeContent.push({
        type: "document",
        source: { type: "base64", media_type: "application/pdf", data: base64Pdf },
      });

      log(`üì§ Calling Claude Sonnet for PDF OCR...`);

      // PDF processing continues here (only reached for non-Excel files)
      const message = await anthropic.messages.create({
        model: "claude-sonnet-4-5-20250929",
        max_tokens: 4096,
        messages: [
          {
            role: "user",
            content: [
              ...claudeContent as any,
              {
                type: "text",
                text: `Analysera PDF-dokumentet.
                
                INSTRUKTIONER:
                1. Hitta Metadata (Leverant√∂r, Datum, Adress).
                2. Extrahera alla rader du kan hitta fr√•n dokumentet.
                
                ‚ö†Ô∏è KRITISKT - DATUM/PERIOD-HANTERING:
                Om dokumentet visar en PERIOD (datumintervall), extrahera ALLTID SLUTDATUMET!
                Exempel:
                - "Period 20251201-20251231" ‚Üí anv√§nd "2025-12-31" (slutdatum!)
                - "Period: 2025-12-01 - 2025-12-31" ‚Üí anv√§nd "2025-12-31" (slutdatum!)
                Slutdatumet representerar n√§r arbetet SLUTF√ñRDES ("Utf√∂rtdatum").
                
                JSON OUTPUT:
                {
                  "date": { "value": "YYYY-MM-DD", "confidence": Number },
                  "supplier": { "value": "String", "confidence": Number },
                  "weightKg": { "value": Number, "confidence": Number },
                  "cost": { "value": Number, "confidence": Number },
                  "totalCo2Saved": { "value": Number, "confidence": Number },
                  "material": { "value": "String (Huvudkategori)", "confidence": Number },
                  "address": { "value": "String", "confidence": Number },
                  "receiver": { "value": "String", "confidence": Number },
                  "lineItems": [
                    {
                      "material": { "value": "String", "confidence": Number },
                      "handling": { "value": "String", "confidence": Number },
                      "weightKg": { "value": Number, "confidence": Number },
                      "co2Saved": { "value": Number, "confidence": Number },
                      "percentage": { "value": "String", "confidence": Number },
                      "isHazardous": { "value": Boolean, "confidence": Number },
                      "address": { "value": "String", "confidence": Number },
                      "receiver": { "value": "String", "confidence": Number }
                    }
                  ]
                }
                Returnera ENDAST ren JSON.`,
              },
            ],
          },
        ],
      });

      log(`‚úì Claude response received`);

      const textContent = message.content[0].type === 'text' ? message.content[0].text : "";
      let rawData = extractJsonFromResponse(textContent);
      
      log(`‚úì JSON parsed successfully`);

      const validatedData = WasteRecordSchema.parse({
          ...rawData,
          lineItems: rawData.lineItems || [],
          _processingLog: processingLog  // ‚úÖ Include processing log
      });
      
      const lineItemCount = validatedData.lineItems?.length || 0;
      log(`‚úÖ PDF extraction complete: ${lineItemCount} line items extracted`);
      log(`${"=".repeat(60)}`);

      await supabase.from("documents").update({
        status: "needs_review",
        extracted_data: {
          ...validatedData,
          _processingLog: processingLog  // ‚úÖ Include processing log in saved data
        }
      }).eq("id", documentId);
      
      revalidatePath("/");
      return;
    }

  } catch (error: any) {
    console.error("‚ùå Process Fail:", error);
    await supabase.from("documents").update({ status: "error" }).eq("id", documentId);
    throw error;
  }
}

/**
 * RE-VERIFY DOCUMENT (AI Dubbelkoll)
 * Reruns AI extraction on an existing document
 * @param customInstructions - Optional extra instructions from user to guide the AI
 */
export async function reVerifyDocument(documentId: string, customInstructions?: string) {
  const supabase = createServiceRoleClient();
  const { data: doc } = await supabase.from("documents").select("*").eq("id", documentId).single();
  if (!doc) throw new Error("Dokument hittades inte");

  await supabase.from("documents").update({ status: "processing" }).eq("id", documentId);

  try {
    const { data: fileData, error: downloadError } = await supabase.storage
      .from("raw_documents")
      .download(doc.storage_path);

    if (downloadError) throw new Error("Kunde inte ladda ner fil");
    const arrayBuffer = await fileData.arrayBuffer();
    
    let claudeContent = [];
    let calculatedTotals = { weight: 0, cost: 0, co2: 0, hazardousCount: 0 };
    let isBigFile = false;

    if (doc.filename.endsWith(".xlsx") || doc.filename.endsWith(".xls")) {
      // EXCEL: Use adaptive extraction for ALL rows from ALL sheets (re-verify)
      const workbook = XLSX.read(arrayBuffer);
      
      // ‚úÖ FIX: Process ALL sheets, not just the first one!
      console.log(`üìä Re-verify: Excel has ${workbook.SheetNames.length} sheet(s): ${workbook.SheetNames.join(', ')}`);
      
      let allData: any[][] = [];
      
      for (const sheetName of workbook.SheetNames) {
        const sheet = workbook.Sheets[sheetName];
        const sheetData = XLSX.utils.sheet_to_json(sheet, { header: 1, defval: "" }) as any[][];
        
        if (sheetData.length === 0) continue;
        
        console.log(`   üìÑ Sheet "${sheetName}": ${sheetData.length} rows`);
        
        if (allData.length === 0) {
          allData = sheetData;
        } else {
          const firstRowLooksLikeHeader = sheetData[0]?.some((cell: any) => 
            String(cell).toLowerCase().match(/datum|material|vikt|adress|kvantitet/)
          );
          allData = [...allData, ...(firstRowLooksLikeHeader && sheetData.length > 1 ? sheetData.slice(1) : sheetData)];
        }
      }
      
      const jsonData = allData;
      console.log(`üìä Using adaptive extraction for ${jsonData.length} rows from all sheets (re-verify)...`);
      
      // Get settings (or use defaults)
      const { data: settingsData } = await supabase
        .from("settings")
        .select("*")
        .single();
      
      // Merge custom instructions into settings if provided
      const settings = {
        ...(settingsData || {}),
        custom_instructions: customInstructions || settingsData?.custom_instructions
      };

      // Use adaptive extraction for ALL rows
      const adaptiveResult = await extractAdaptive(
        jsonData as any[][],
        doc.filename,
        settings
      );

      // Convert to expected format
      const extractedData = {
        ...adaptiveResult,
        totalCostSEK: 0,
        documentType: "waste_report",
      };

      // Determine status based on extraction quality
      const qualityScore = (adaptiveResult._validation.completeness + (adaptiveResult.metadata?.confidence || 0) * 100) / 2;
      const status = qualityScore >= 90 ? "approved" : "needs_review";
      
      await supabase.from("documents").update({
        status,
        extracted_data: extractedData,
        updated_at: new Date().toISOString()
      }).eq("id", documentId);

      revalidatePath(`/review/${documentId}`);
      revalidatePath("/");
      return;

    } else {
      // PDF: Keep existing Claude Vision processing with logging
      const processingLog: string[] = [];
      const log = (msg: string) => {
        const ts = new Date().toISOString().split('T')[1].split('.')[0];
        processingLog.push(`[${ts}] ${msg}`);
        console.log(msg);
      };
      
      log(`${"=".repeat(60)}`);
      log(`üìÑ PDF RE-VERIFICATION: ${doc.filename}`);
      log(`${"=".repeat(60)}`);
      if (customInstructions) {
        log(`üìù Custom instructions provided`);
      }
      
      const base64Pdf = Buffer.from(arrayBuffer).toString("base64");
      log(`‚úì PDF converted to base64 (${(arrayBuffer.byteLength / 1024).toFixed(0)} KB)`);
      
      claudeContent.push({
        type: "document",
        source: { type: "base64", media_type: "application/pdf", data: base64Pdf },
      });

      log(`üì§ Calling Claude Sonnet for PDF OCR...`);

      // PDF processing continues here (only reached for non-Excel files)
      const message = await anthropic.messages.create({
        model: "claude-sonnet-4-5-20250929",
        max_tokens: 4096,
        messages: [
          {
            role: "user",
            content: [
              ...claudeContent as any,
              {
                type: "text",
                text: `Du √§r en expert-AI f√∂r avfallsrapporter. Analysera PDF-dokumentet noggrant.

                ANV√ÑND DESSA SYNONYMER F√ñR ATT HITTA R√ÑTT KOLUMN:
                - Material: "BEAst-artikel", "Fraktion", "Avfallsslag", "Artikel", "Taxekod", "Restprodukt".
                - Adress: "H√§mtadress", "Littera", "Arbetsplatsnamn", "Uppdragsst√§lle", "Anl√§ggningsadress".
                - Vikt: "Vikt (kg)", "M√§ngd", "Kvantitet", "Antal kg", "Vikt k√∂rtur".
                - Farligt Avfall: Leta efter texten "Farligt avfall", "FA" eller material som Asbest, Elektronik, Batterier, Kemikalier.
                
                INSTRUKTIONER:
                1. Hitta Metadata (Leverant√∂r, Datum, Adress).
                2. Extrahera alla rader du kan hitta fr√•n dokumentet.
                3. Farligt avfall: S√§tt "isHazardous": true om det √§r elektronik, kemikalier, asbest etc.
                4. Adress per rad: Om tabellen har kolumner som "H√§mtst√§lle", "Littera" eller "Projekt", extrahera dessa per rad.
                
                ‚ö†Ô∏è KRITISKT - DATUM/PERIOD-HANTERING:
                Om dokumentet visar en PERIOD (datumintervall), extrahera ALLTID SLUTDATUMET!
                Exempel:
                - "Period 20251201-20251231" ‚Üí anv√§nd "2025-12-31" (slutdatum!)
                - "Period: 2025-12-01 - 2025-12-31" ‚Üí anv√§nd "2025-12-31" (slutdatum!)
                Slutdatumet representerar n√§r arbetet SLUTF√ñRDES ("Utf√∂rtdatum").
${customInstructions ? `
                EXTRA INSTRUKTIONER FR√ÖN ANV√ÑNDAREN (H√ñGSTA PRIORITET):
                ${customInstructions}
` : ''}
                JSON OUTPUT:
                {
                  "date": { "value": "YYYY-MM-DD", "confidence": Number },
                  "supplier": { "value": "String", "confidence": Number },
                  "weightKg": { "value": Number, "confidence": Number },
                  "cost": { "value": Number, "confidence": Number },
                  "totalCo2Saved": { "value": Number, "confidence": Number },
                  "material": { "value": "String (Huvudkategori)", "confidence": Number },
                  "address": { "value": "String", "confidence": Number },
                  "receiver": { "value": "String", "confidence": Number },
                  "lineItems": [
                    {
                      "material": { "value": "String", "confidence": Number },
                      "handling": { "value": "String", "confidence": Number },
                      "weightKg": { "value": Number, "confidence": Number },
                      "co2Saved": { "value": Number, "confidence": Number },
                      "percentage": { "value": "String", "confidence": Number },
                      "isHazardous": { "value": Boolean, "confidence": Number },
                      "address": { "value": "String", "confidence": Number },
                      "receiver": { "value": "String", "confidence": Number }
                    }
                  ]
                }
                Returnera ENDAST ren JSON.`,
              },
            ],
          },
        ],
      });

      log(`‚úì Claude response received`);

      const textContent = message.content[0].type === 'text' ? message.content[0].text : "";
      let rawData = extractJsonFromResponse(textContent);
      
      log(`‚úì JSON parsed successfully`);

      const validatedData = WasteRecordSchema.parse({
          ...rawData,
          lineItems: rawData.lineItems || [],
          _processingLog: processingLog  // ‚úÖ Include processing log
      });
      
      const lineItemCount = validatedData.lineItems?.length || 0;
      log(`‚úÖ PDF re-verification complete: ${lineItemCount} line items extracted`);
      log(`${"=".repeat(60)}`);

      await supabase.from("documents").update({
        status: "needs_review",
        extracted_data: {
          ...validatedData,
          _processingLog: processingLog  // ‚úÖ Include processing log in saved data
        }
      }).eq("id", documentId);
      
      revalidatePath(`/review/${documentId}`);
      revalidatePath("/");
      return;
    }

    revalidatePath(`/review/${documentId}`);
    revalidatePath("/");

  } catch (error: any) {
    console.error("‚ùå Re-Verify Fail:", error);
    await supabase.from("documents").update({ status: "error" }).eq("id", documentId);
    throw error;
  }
}

// ... (Beh√•ll saveDocument, deleteDocument etc) ...
export async function saveDocument(formData: FormData) {
  const supabase = createServiceRoleClient();
  const id = formData.get("id") as string;
  
  // Get existing document to preserve all data
  const { data: existingDoc } = await supabase
    .from("documents")
    .select("*")
    .eq("id", id)
    .single();
  
  if (!existingDoc) {
    throw new Error("Document not found");
  }
  
  const existingData = existingDoc.extracted_data || {};
  const existingLineItems = existingData.lineItems || [];
  
  // Get edited document metadata from form
  const editedDate = formData.get("date") as string;
  const editedSupplier = formData.get("supplier") as string;
  const editedAddress = formData.get("address") as string;
  const editedReceiver = formData.get("receiver") as string;
  
  // Get lineItems from form, MERGING with existing data to preserve extra fields
  const lineItems: any[] = [];
  let index = 0;
  while (formData.get(`lineItems[${index}].material`) !== null) {
    const material = formData.get(`lineItems[${index}].material`) as string;
    const weightKg = parseFloat(formData.get(`lineItems[${index}].weightKg`) as string || "0");
    const address = formData.get(`lineItems[${index}].address`) as string;
    const location = formData.get(`lineItems[${index}].location`) as string;
    const receiver = formData.get(`lineItems[${index}].receiver`) as string;
    const handling = formData.get(`lineItems[${index}].handling`) as string;
    const isHazardous = formData.get(`lineItems[${index}].isHazardous`) === "true";
    const co2Saved = parseFloat(formData.get(`lineItems[${index}].co2Saved`) as string || "0");
    // ‚úÖ NEW: Get row-specific date
    const rowDate = formData.get(`lineItems[${index}].date`) as string;
    
    if (material || weightKg > 0) {
      // PRESERVE: Start with original line item data (if exists) to keep extra fields
      // like wasteCode, costSEK, referensnummer, fordon, unit, etc.
      const originalItem = existingLineItems[index] || {};
      
      // Merge: original fields + edited fields (edited fields take priority)
      lineItems.push({
        ...originalItem, // Keep ALL original fields (wasteCode, costSEK, unit, etc.)
        // Override with edited values from form:
        material: { value: material || "", confidence: 1 },
        weightKg: { value: weightKg, confidence: 1 },
        address: address ? { value: address, confidence: 1 } : originalItem.address,
        location: location ? { value: location, confidence: 1 } : originalItem.location,
        receiver: receiver ? { value: receiver, confidence: 1 } : originalItem.receiver,
        handling: handling ? { value: handling, confidence: 1 } : originalItem.handling,
        isHazardous: { value: isHazardous, confidence: 1 },
        co2Saved: co2Saved > 0 ? { value: co2Saved, confidence: 1 } : originalItem.co2Saved,
        // ‚úÖ NEW: Save row-specific date (critical for export!)
        date: rowDate ? { value: rowDate, confidence: 1 } : originalItem.date,
      });
    }
    index++;
  }
  
  // Get totals
  const totalCo2Saved = parseFloat(formData.get("totalCo2Saved") as string || "0");
  const weightKg = parseFloat(formData.get("weightKg") as string || "0");
  const cost = parseFloat(formData.get("cost") as string || "0");
  
  // Calculate total weight from lineItems if not provided
  const calculatedWeight = lineItems.reduce(
    (sum, item) => sum + (Number(item.weightKg?.value) || 0),
    0
  );
  const finalWeight = weightKg || calculatedWeight;
  
  // Update extracted_data with edited values
  const updatedData = {
    ...existingData,
    lineItems,
    totalWeightKg: finalWeight,
    totalCostSEK: cost,
    totalCo2Saved,
    // Update document-level metadata with edited values
    documentMetadata: {
      date: editedDate || existingData.documentMetadata?.date || "",
      supplier: editedSupplier || existingData.documentMetadata?.supplier || "",
      address: editedAddress || existingData.documentMetadata?.address || "",
      receiver: editedReceiver || existingData.documentMetadata?.receiver || "",
    },
    // Also update top-level fields for backward compatibility
    date: { value: editedDate || "", confidence: 1 },
    supplier: { value: editedSupplier || "", confidence: 1 },
    address: { value: editedAddress || "", confidence: 1 },
    receiver: { value: editedReceiver || "", confidence: 1 },
  };
  
  await supabase
    .from("documents")
    .update({
      extracted_data: updatedData,
      status: "approved",
      updated_at: new Date().toISOString(),
    })
    .eq("id", id);
  
  revalidatePath("/collecct");
  revalidatePath(`/review/${id}`);
  
  // Return success - let the client handle navigation
  return { success: true, documentId: id };
}
// (Beh√•ll √∂vriga exporterade funktioner)
export async function deleteDocument(formData: FormData) {
    const supabase = createServiceRoleClient();
    const id = formData.get("id") as string;
    const storagePath = formData.get("storagePath") as string;
    if (storagePath) await supabase.storage.from(STORAGE_BUCKET).remove([storagePath]);
    await supabase.from("documents").delete().eq("id", id);
    revalidatePath("/");
    revalidatePath("/archive");
    revalidatePath("/collecct");
  }
export async function toggleArchive(formData: FormData) {
    const supabase = createServiceRoleClient();
    const id = formData.get("id") as string;
    const currentState = formData.get("currentState") === "true"; 
    await supabase.from("documents").update({ archived: !currentState }).eq("id", id);
    revalidatePath("/");
    revalidatePath("/archive");
}
export async function addMaterial(formData: FormData) {
    const supabase = createServiceRoleClient();
    const name = formData.get("name") as string;
    if (!name) return;
    const formattedName = name.charAt(0).toUpperCase() + name.slice(1).toLowerCase();
    await supabase.from("materials").insert({ name: formattedName });
    revalidatePath("/settings");
    revalidatePath("/review/[id]", "page"); 
  }
export async function deleteMaterial(formData: FormData) {
    const supabase = createServiceRoleClient();
    const id = formData.get("id") as string;
    await supabase.from("materials").delete().eq("id", id);
    revalidatePath("/settings");
    revalidatePath("/review/[id]", "page");
}

/**
 * RETRY PROCESSING
 * Retries processing a document that failed (status = "error")
 */
export async function retryProcessing(documentId: string) {
  const supabase = createServiceRoleClient();
  const { data: doc } = await supabase.from("documents").select("*").eq("id", documentId).single();
  if (!doc) throw new Error("Dokument hittades inte");
  
  // Bara till√•t retry om dokumentet har status "error"
  if (doc.status !== "error") {
    throw new Error("Kan bara f√∂rs√∂ka igen p√• dokument med fel-status");
  }

  // √Öterst√§ll status och k√∂r processDocument igen
  await supabase.from("documents").update({ status: "uploaded" }).eq("id", documentId);
  
  try {
    await processDocument(documentId);
    revalidatePath("/");
    revalidatePath("/archive");
  } catch (error) {
    // Om det fortfarande misslyckas, s√§tt tillbaka till error
    await supabase.from("documents").update({ status: "error" }).eq("id", documentId);
    throw error;
  }
}

/**
 * ARKIVERA ALLA DOKUMENT
 * S√§tter archived = true p√• alla dokument som inte redan √§r arkiverade
 */
export async function archiveAllDocuments() {
  const supabase = createServiceRoleClient();
  
  // Uppdatera alla dokument som INTE √§r arkiverade
  const { error } = await supabase
    .from("documents")
    .update({ archived: true })
    .eq("archived", false); // P√•verkar bara den aktiva listan

  if (error) {
    console.error("Archive All Error:", error);
    throw new Error("Kunde inte arkivera allt.");
  }

  revalidatePath("/");
  revalidatePath("/archive");
}

/**
 * GODK√ÑNN ALLA DOKUMENT
 * S√§tter status = "approved" p√• alla dokument som beh√∂ver granskas
 */
export async function verifyAllDocuments() {
  const supabase = createServiceRoleClient();
  
  // Uppdatera alla dokument som beh√∂ver granskas eller √§r i processing
  const { error } = await supabase
    .from("documents")
    .update({ status: "approved" })
    .in("status", ["needs_review", "processing", "uploaded", "queued"]);

  if (error) {
    console.error("Verify All Error:", error);
    throw new Error("Kunde inte godk√§nna allt.");
  }

  revalidatePath("/");
  revalidatePath("/review/[id]", "page");
}

/**
 * REJECT DOCUMENT (Collecct workflow)
 * Rejects a document for manual processing
 */
export async function rejectDocument(formData: FormData) {
  const supabase = createServiceRoleClient();
  const id = formData.get("id") as string;
  const reason = formData.get("reason") as string | null;

  // Get current extracted_data
  const { data: currentDoc } = await supabase
    .from("documents")
    .select("extracted_data")
    .eq("id", id)
    .single();

  await supabase
    .from("documents")
    .update({
      status: "rejected",
      extracted_data: {
        ...(currentDoc?.extracted_data || {}),
        rejected: true,
        rejected_at: new Date().toISOString(),
        rejection_reason: reason || "Manual rejection",
      },
    })
    .eq("id", id);

  revalidatePath("/collecct");
  revalidatePath("/");
}